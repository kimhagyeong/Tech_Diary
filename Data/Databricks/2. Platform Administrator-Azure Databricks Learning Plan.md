
# 0. What is the Databricks Lakehouse Platform?

### 1. What is Databricks?
Databricks는 Data, AI 회사로 알려져 있다. Databricks는 SaaS 를 제공하며, 빅데이터와 ai를 쉽게 매니징하고 모든 기업에서 데이터 혁신을 제공한다.    

- Ingest, process, and transform massive quantities and types of data
- Explore data through data science techniques, including but not limited to machine learning
- Guarantee that data available for business queries is reliable and up to date 
- Provide data engineers, data scientists, and data analysts the unique tools they need to do their work
- Overcome traditional challenges associated with data science and machine learning workflows (we will explore this in detail in our next lesson)

기존에는 big data 를 돌리기 위해서 많은 노력이 필요했다. 
Data warehousing(structured data -> data warehouse -> data markt->analytics and bi) -> Data Engineering (data -> data lake (extract, transform, load..) || hadoop, apache spark, cloudera..)-> Streaming (apache kafka, apache spark)-> data science & Machine Learning....(jupyter..) 등등등.. 아주 복잡해땅. 서로 툴들이 다르니까..


### 2. Datahouse, datalake, lakehouse.


### 3. Delta lake
---------

# 1. What is Delta Lake?

### 1. Why Delta Lake?
Delta Lake is an open format storage layer that puts standards in place in an organization’s data lake to provide data structure and governance. These standards ensure that all the data stored in a data lake is reliable and ready to use for business needs. It was developed at Databricks and open-sourced in early 2019.

Delta Lake was created by Databricks to help organizations address challenges with storing and managing big data using data lakes


Data Lakes : could handle all your data for data science and ML.
sotring all types of data (structred, unstructured, and semi-structured.
sotre data relatively cheaply compared to data warehouses.
 however..... 
- poor BI support
- complex to set up
- poor performance
- unreliable data swamps
때문에 delta lake 를 만듦

. Because they store all data types for long periods, data lakes can quickly become data swamps. Data swamps are data lakes that are difficult to navigate and manage. Delta Lake was designed to bring the governance and structure of data warehouses into data lakes to, above all else, ensure that the data in an organization’s data lake is reliable for use in big data and AI projects.


### 2. How is Delta Lake used to Organize Data?
Delta lake 는 아키텍쳐 패턴에 따라서 데이터를 정리하여 successively cleaner table 로 만든다. bronze table -> siver tables -> gold table 순으로 data quality 를 올리는데, bronze 는 raw한 데이터 json, RDBMS, iot data 같은거고 gold 는 비지니스에 사용될만 한 버전의 데이터이다.

### 3. Elements of Delta Lake
#### Delta files
delta lake 는 customer의 데이터를 cloud storage 계정으로 저장한 Parquet file을 사용한다. Parquet 는 
Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON. The columnar storage allows you to quickly skip over non-relevant data while executing queries.
Delta files leverage all of the technical capabilities of Parquet files but have an additional layer over them. This additional layer tracks data versioning and metadata, stores transaction logs to keep track of changes made to a data table or object storage directory, and provides ACID transactions.

#### Delta tables
delta table registred in a Metastore
Delta Tables
A Delta table is a collection of data kept using the Delta Lake technology and consists of three things:
Delta files containing the data and kept in object storage
A Delta table registered in a Metastore (a metastore is simply a catalog that tracks your data’s metadata - data about your data)
The Delta Transaction Log saved with Delta files in object storage


#### Delta Optimization Engine

Delta Engine is a high-performance query engine that provides an efficient way to process data in data lakes . What the Delta Optimization Engine means for your business is that your data workloads run faster, so data times can perform their work in less time

#### Delta Lake storage layer

When using Delta Lake, your organization stores its data in a Delta Lake Storage Layer and then accesses that data via Databricks. A key idea here is that an organization keeps all of this data in files in object storage. This is beneficial because it means your data is kept in a lower-cost and easily scalable environment.



간단히 말해 아래 기능을 제공한다.
- One platform for every use case, 
- High performance query engine
- structured transactional layer
- Data lake for all your data

data lake 에는 너어어무 많은 문제가 있어.
delta lake에서는 아래 방법으로 해결했지
- ACID Transactions
- Schema management : Columns that are present in the table but not in the data are set to null. If there are extra columns in the data that are not present in the table, this operation throws an exception. This ensures that bad data that could corrupt your system is not written into it. Delta Lake also enables you to make changes to a table’s schema that can be applied automatically.
- scalable metadata handling : 분산 처리를 이용해서 큰 메타데이터도 일반 데이터처럼 처리함.
- unified batch and streaming data 
- Data versioning and time travel


정리해서 delta lake 의 장점은
- improve ETL pipeline
- unify batch and streaming (Apache spark 로 통합)
- BI on your data lake
- Meet regulatory needs (keep record of historical data change)
