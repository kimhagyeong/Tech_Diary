## 1. 회귀 분석
- 단순 선형 회구 분석 : 𝑌=𝛽0+𝛽1𝑋+𝜀
- 실제 값과 우리가 추정한 값의 차이(= 잔차 residual)가 적으면 적을 수록 좋음
- 잔차의 제곱 합은 SSE 이며 미분 가능
- <img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9.png" width="700"/>
- <img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9%20%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1.png" width="700"/>
<br/><br/>
```
import os
import pandas as pd 
import numpy as np
import statsmodels.api as sm
```
```
sm.add_constant({{value}}, has_constant='add')
sm.OLS({{target value}},{{statemodle value}})            
sm.OLS({{target value}},{{statemodle value}}).fit()
sm.OLS({{target value}},{{statemodle value}}).summary()
sm.OLS({{target value}},{{statemodle value}}).predict({{value}})
```
OLS는 가장 기본적인 결정론적 회귀 방법으로  
잔차제곱합(RSS)를 최소화하는 가중치 벡터를 행렬미분으로 구하는 방법
```
(np.dot(crim1,fitted_model1.params))
```
np.dot 은 내적 구하기  

```
import matplotlib.pyplot as plt
```
matplotlib는 시각화!  
참고 : https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html    
https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=sanghan1990&logNo=221116465873

<br/><br/><br/><br/>

## 2. 다중 선형 회귀분석
- 변수가 여러개인 경우
- 귀무가설 = 0 : 회귀 계수는 0이다. 변수의 설명력이 없다.  
  이렇게 되면 대립가설 != 0 회귀 계수가 0이 아니다. 변수의 설명력이 존재한다.  
- 귀무가설 : B1 = B2 ... Bp = 0 모든 회귀 계수는 0이다. 변수의 설명력이 하나도 존재 하지 않는다.  
  이렇게 되면 대립가설 = 하나의 회귀 계수라도 0이 아니다. 설명력이 있는 변수가 존재한다는 의미.  
  -> 기각 하기 쉬운 가설. 변수가 추가되면 추가될 수록 기각하기 쉬워진다.  
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9%20%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1.png" width="700"/>   

총 변동은 정해져 있기 때문에 변수가 추가되면 추가될 수록 SSR이 많아지기 때문에 R이 증가.<br/>
- 다중회귀분석 실습
```
x_data=Array[['A','B','C']]
sm.add_constant(x_data, has_constant='add')
sm.OLS({{target value}},{{statemodle value}})            
sm.OLS({{target value}},{{statemodle value}}).fit()
sm.OLS({{target value}},{{statemodle value}}).summary()
sm.OLS({{target value}},{{statemodle value}}).predict({{value}})
```
OLS의 첫번째 변수로 배열을 넣으면 됨.   
OLS이 아닌 행렬의 역행렬을 통해서 다중회귀분석 값을 구할 수 있다.   
```
from numpy import linalg 
linalg.inv((np.dot(x_data1.T,x_data1)))
```
<br/><br/>
참고 :https://todayisbetterthanyesterday.tistory.com/7
<br/><br/><br/>

- 다중 공선성 : 독립 변수들이 강한 선형관계에 있을 때  
  즉, 독립 변수들의 변동성이 겹치게 되면 겹치는 변동성에 대해서 중복으로 가져가지 못하기 때문에   
  잘못된 변수해석, 예측 정확도 하락등을 야기시킨다.  
- 즉 중요한 변수(Feature)를 뽑는 것이 중요한 이슈  
- 다중공성선 해결 방법 :   
  Feature Selection(중요 변수만 해결) : Lasso, stepwise, 유전 알고리즘.  
  변수를 줄이지 않고 활용 : AutoEncoder, PCA, Ridge.   
- 실습   
다중회귀분석을 할때 array 안에 변수들을 더 많이 추가해 보자.  
그리고 나서 sm.OLS의 summary를 돌리면 Condition Number 를 의미하는 Cond. No 의 값이 크게 나온 것을 확인할 수 있다.  
변수가 적절한지 확인할 때 이처럼 Cond. No를 확인하는 것과   
p-value가 0.05보다 큰지 확인한다.  
```
sm.OLS({{target value}},{{statemodle value}}).corr()
```
corr = correlation 상관행렬   
단순상관분석 : 단순히 두 개의 변수가 어느 정도 강한 관계에 있는가를 측정    
결과값의 크기를 보고 정도를 판단하는데, 결과값을 r 이라고 한다.    
r = X와 Y가 함께변하는 정도 / X와 Y가 각각 변하는 정도    
일반적으로
```
r이 -1.0과 -0.7 사이이면, 강한 음적 선형관계,
r이 -0.7과 -0.3 사이이면, 뚜렷한 음적 선형관계,
r이 -0.3과 -0.1 사이이면, 약한 음적 선형관계,
r이 -0.1과 +0.1 사이이면, 거의 무시될 수 있는 선형관계,
r이 +0.1과 +0.3 사이이면, 약한 양적 선형관계,
r이 +0.3과 +0.7 사이이면, 뚜렷한 양적 선형관계,
r이 +0.7과 +1.0 사이이면, 강한 양적 선형관계
```
로 해석하며 우리는 corr의 결과값이 +-0.5 이상이면 다중공선성 발생을 의심한다.    

- 회귀모델의 성능지표 : R2, Adjusted R2, AIC, BIC
- 모형의 성능지표 : MSE, MAPE    
  정밀도(precision) - 분류 모형이 불량을 진단하기 위해 얼마나 잘 작동했는지 보여주는 지표.  
  재현율(recall) - 불량 데이터중 실제로 불량이라고 진단한 제품의 비율(진단확률).  
  특이도(specificity) - 분류 모형이 정상을 진단하기 위해 잘 작동하는지를 보여주는 지표.  
  ROC curve, AUC
- 변수 선택법  
  Feedforward Selection : 변수를 추가해가며 성능지표를 비교해가는 방법
  Backward Elimination : 변수를 제거해가며 성능지표를 비교해가는 방법
  Stepwise : 가장 유의한 변수를 추가하거나 유의하지 않는 변수를 제거하는 방법
  
<br/><br/><br/><br/>

