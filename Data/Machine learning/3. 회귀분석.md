## 1. 회귀 분석
- 단순 선형 회구 분석 : 𝑌=𝛽0+𝛽1𝑋+𝜀
- 실제 값과 우리가 추정한 값의 차이(= 잔차 residual)가 적으면 적을 수록 좋음
- 잔차의 제곱 합은 SSE 이며 미분 가능
- <img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9.png" width="700"/>
- <img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9%20%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1.png" width="700"/>
<br/><br/>
```
import os
import pandas as pd 
import numpy as np
import statsmodels.api as sm
```
```
sm.add_constant({{value}}, has_constant='add')
sm.OLS({{value}},{{statemodle value}})            
sm.OLS({{value}},{{statemodle value}}).fit()
sm.OLS({{value}},{{statemodle value}}).summary()
sm.OLS({{value}},{{statemodle value}}).predict({{value}})
```
OLS는 가장 기본적인 결정론적 회귀 방법으로  
잔차제곱합(RSS)를 최소화하는 가중치 벡터를 행렬미분으로 구하는 방법
```
(np.dot(crim1,fitted_model1.params))
```
np.dot 은 내적 구하기  

```
import matplotlib.pyplot as plt
```
matplotlib는 시각화!  
참고 : https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html    
https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=sanghan1990&logNo=221116465873

<br/><br/><br/><br/>

## 2. 다중 선형 회귀분석
- 변수가 여러개인 경우
- 귀무가설 = 0 : 회귀 계수는 0이다. 변수의 설명력이 없다.  
  이렇게 되면 대립가설 != 0 회귀 계수가 0이 아니다. 변수의 설명력이 존재한다.  
- 귀무가설 : B1 = B2 ... Bp = 0 모든 회귀 계수는 0이다. 변수의 설명력이 하나도 존재 하지 않는다.  
  이렇게 되면 대립가설 = 하나의 회귀 계수라도 0이 아니다. 설명력이 있는 변수가 존재한다는 의미.  
  -> 기각 하기 쉬운 가설. 변수가 추가되면 추가될 수록 기각하기 쉬워진다.  
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%92%E1%85%AA%E1%86%A8%E1%84%83%E1%85%A9%20%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1.png" width="700"/>   
총 변동은 정해져 있기 때문에 변수가 추가되면 추가될 수록 SSR이 많아지기 때문에 R이 증가    
- 다중 공선성 : 독립 변수들이 강한 선형관계에 있을 때. 
  즉, 독립 변수들의 변동성이 겹치게 되면 겹치는 변동성에 대해서 중복으로 가져가지 못하기 때문에   
  잘못된 변수해석, 예측 정확도 하락등을 야기시킨다.
- 즉 중요한 변수(Feature)를 뽑는 것이 중요한 이슈
- 다중공성선 해결 방법 :   
  Feature Selection(중요 변수만 해결) : Lasso, stepwise, 유전 알고리즘.  
  변수를 줄이지 않고 활용 : AutoEncoder, PCA, Ridge.   

- 회귀모델의 성능지표 : R2, Adjusted R2, AIC, BIC
- 모형의 성능지표 : MSE, MAPE    
  정밀도(precision) - 분류 모형이 불량을 진단하기 위해 얼마나 잘 작동했는지 보여주는 지표.  
  재현율(recall) - 불량 데이터중 실제로 불량이라고 진단한 제품의 비율(진단확률).  
  특이도(specificity) - 분류 모형이 정상을 진단하기 위해 잘 작동하는지를 보여주는 지표.  
  ROC curve, AUC
- 변수 선택법  
  Feedforward Selection : 변수를 추가해가며 성능지표를 비교해가는 방법
  Backward Elimination : 변수를 제거해가며 성능지표를 비교해가는 방법
  Stepwise : 가장 유의한 변수를 추가하거나 유의하지 않는 변수를 제거하는 방법
  
<br/><br/><br/><br/>

