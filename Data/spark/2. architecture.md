## Apache Spark Architecture – Spark Cluster Architecture Explained

spark를 사용하다가 에러를 보게 되면 눈에 익는 단어들이 생긴다. (DAG, Excutor, shuffle 등)    
각 단어들은 spark 를 구성하는 아키텍처 요소들이다. 지금부터 spark 의 architecture 를 잘 설명해준 블로그를 번역하고자 한다.



출처 : https://www.edureka.co/blog/spark-architecture/



Apache spark 는 빅데이터 세계에서 화두가 되고 있는 오픈소스 클러스터 컴퓨팅 프레임워크이다. spark 전문가에 따르면, hadoop 에 비해 100배 정도 메모리가 빠르고 10배정도 디스크가 빠르다고 한다. 이 블로그에서 spark architecture 와 기초에 대해서 간단히 소개하고자 한다.

<br/><br/>

### 이 글에서의 주제
- Spark & its Features
- Spark Architecture Overview
- Spark Eco-System
- Resilient Distributed Datasets (RDDs)
- Working of Spark Architecture
- Example using Scala in Spark Shell


<br/><br/>
## 1. Spark & its Features
스파크와 그 특징    
Apache Spark 는 실시간으로 데이터를 처리하는 오픈소스 클러스터 컴퓨팅 프레임워크이다. Apache spark 의 가장 큰 특징은 어플리케이션의 processing  속도를 높일 수 있는 in-memory cluster computing 를 사용한다는 것이다. (RAM 에 데이터를 저장, 디스크에 저장하는 것보다 내부 최적화 알고리즘이 더 단순하며 더 적은 CPU 명령을 실행하기 때문에 빠르다)    
spark 는 모든 클러스터가 병렬로 처리되고 작업 실패시 재작업을 보장하는 인터페이스를 제공한다. spark 는 batch applications, 반복 algorithms, 대화형 쿼리, streaming과 같은 넓은 작업 범위를 커버하도록 디자인 되어 있다.

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_1.png" width="700"/>

1. speed    
spark 는 대용량 데이터 프로세싱을 하는 Hadoop MapReduce 보다 100배는 빠르다. 파티셔닝을 함으로써 속도를 향상 시킨다.
2. Powerful Caching    
간단한 프로그래밍 계층에서 강력한 캐싱과 디스크 영구 저장을 제공한다. (장치에 전원이 꺼져도 데이터 유지. 비휘발성을 의미. 앞서 RAM을 사용한다고 했으니 보편적인 휘발성 RAM 이 아닌 비휘발성 RAM을 지원한다는 것을 의미)
3. Deployment    
spark는 Mesos, YARN을 사용한 Hadoop, spark 자체 클러스터 메니저를 통해 배포할 수 있습니다.
4. Real-Time    
spark는 in-memory 컴퓨팅을 통해 실시간 컴퓨팅과 적은 지연시간을 제공합니다.
5. Polyglot (여러 언어 가능)    
spark는 높은 레벨의 java, scala, python, R 언어를 지원합니다. 또한 scala, python 에서 shell 도 지원합니다.

<br/><br/>
## 2. Spark Architecture Overview
Apache Spark 는 잘 정의된 아키텍쳐 계층들을 가지고 있는데, 모든 요소들과 계층들은 느슨하게 결합되어 있습니다. 이 아키텍쳐들은 다양한 확장 기능과 라이브러리로 인해 더욱 통합됩니다.




- Resilient Distributed Dataset (RDD) : 탄력적인 분산 데이터 셋
- Directed Acyclic Graph (DAG) : 단방향을 가지는 비순환적인 그래프


<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_2.png" width="700"/>

spark 아키텍처에 대해 깊이 들어가기전에, Eco-system이나 RDD 와 같이 spark 의 간단한 기본 컨셉에 대해 설명하고자 한다.

## 2-1. Spark Eco-System
아래 이미지와 같이, spark echosystem는 다양한 구성요소 (spark SQL, Spark Streaming, MLlib, GraphX,Core API component 등) 으로 구성되어 있다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_3.png" width="700"/>



1. Spark Core     
spark core는 대용량 병렬, 분산 데이터 프로세싱의 기본적인 엔진이다. 나아가, core 맨 꼭데기에 설치된 추가적인 라이브러리들은 스트리밍, sql, 머신러닝을 위해 다양한 작업들을 제공해준다. spark core는 메모리 메니지먼트, 결함 복구, 스케쥴링, 분산처리, Job에 있는 클러스터와 상호작용하는 storage 시스템을 모니터링하는 작업들을 제공한다.
2. Spark Streaming    
Spark Streaming은 실시간 스트리밍 데이터를 프로세싱하는데 사용되는 구성요소이다. 또한 이건 core Spark API에서 유용하게 추가해서 사용한다. spark streaming은 실시간 데이터 스트림에서 높은 처리율과 결함 복구를 보장하는 스트림 작업을 가능하게 한다.
3. Spark SQL    
spark SQL은 spark 에 새로운 모듈인데, 관계형 프로세싱과 함수형 프로그래밍 api를 통합하는 것이다. spark SQL은 SQL 쿼리 데이터 뿐만 아니라 Hive Query 언어까지 제공한다. RDBMS에 익숙한 사용자에게 Spark SQL은 전통적인 관계형 데이터 프로세싱과 같은 기존 툴의 범위에서 쉽게 전환할 수 있을 것이다. 
4. GraphX    
graphX 는 spark api를 위한 그래프와 병렬그래프 컴퓨팅이다. 높은 레벨에서 graphX는 spark RDD와 탄력적인 분산 그래프를 확장한 것이다. 탄력적인 분산 그래프(꼭지점과 변들이 방향이 있는 그래프)
5. MLlib(Machine Learning)    
MLlib은 Machine Learning Library의 기준이고 Apache spark 에서 머신러닝으로 사용된다.
6. SparkR
SparkR은 분산 데이터 프레임 실행을 위해 제공되는  R 패키지이다. sparkR은 large 데이터 셋에서 선별, 필터링, 집합 작업을 지원한다.

<br/><br/>
위와 같이, spark 는 R, SQL, Python, Scala, Java를 지원하는 high-level libraries 패키지이다. 이러한 라이브러리는 복잡한 워크플로우에 매끄럽게 통합되도록 변화해왔다. 뿐만 아니라 spark는 다양한 통합 서비스인 MLlib, GraphX, SQL + Data Frames, Streaming services를 수용가능하게 성장해왔다.




그 다음은 spark의 기본적인 데이터 구조인 RDD에 대해 말하고자 한다.

<br/><br/>

## 2-2. Resilient Distributed Dataset (RDD) : 탄력적인 분산 데이터 셋

모든 Spark 애플리케이션을 block로 만들고 이를 RDD라고 부릅니다.    

- Resilient : 결함 회복. 작업에서 실패한 데이터로부터 재시작할 수 있도록 보장.
- Distributed : 클러스터에 여러 노드들에 있는 분산된 데이터
- Dataset : values 를 가지고 있는 파티션된 모음들.
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_4.png" width="700"/>

RDD 는 분산된 collection들을 추상화한 데이터 계층이다. RDD는 변하지 않고 lazy transformations 을 따른다.



이제 어떻게 동작되는지 궁금해질 텐데, RDD안에 데이터들은 key 를 기준으로 chunk 단위로 나눈다. RDD들은 아주 회복력이 높다. (회복력 : 어떤 이슈가 있던 똑같은 data chunk로 여러 각각의 executor nodes로 복제되고 빠르게 회복한다. 또한, 한 executor node가 실패되도 다른 노드들이 여전히 process 된다.)


게다가, 한번 생성된 RDD는 불변하다. 불변하다는 의미는 한번 생성된 객체는 절대 수정이 불가능하다. 그렇지만 transform 은 가능하다.




분산 환경에 대해 말하자면, RDD에 있는 각각의 dataset은 논리적인 파티션으로 분할되어 있는데 이 파티션들은 클러스터의 서로다른 노드에서 작업된다. 그렇기 떄문에, 사용자는 transformations, actions 등을 완전히 병렬로 작업될 수 있다. spark 에서 분산을 관리하니 걱정할 필요 없다.
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_5.png" width="700"/>

RDD를 생성하는 데는 두 가지 방법이 있다. - driver 에서 collection을 병렬화 하는 방법과 외부 storage system에 데이터 셋을 참조하는 방법이다. (외부 storage system : 공유 file system, HDFS, HBase)



RDD를 통해 유저는 두 가지의 작업을 할 수 있다.
1. Transformations : 새로운 RDD를 생성하는 작업
2. Actions : RDD를 작업하고 결과를 driver로 되돌려주는 작업을 하도록 Apache Spark 에게 알려주는 작업.

<br/><br/>


## 3. Working of Spark Architecture

참고 : https://m.blog.naver.com/PostView.naver?blogId=pjt3591oo&logNo=222724676035&proxyReferer=
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_20.JPG" width="800"/>

Apache spark의 기본 아키텍처에 대해 봐왔다면 더 깊이 동작에 대해 보고자 한다.



사용자의 master node에서는 driver program 이라는 어플리케이션를 이끄는 프로그램이 있다. 쓰고 있는 코드가 드라이버 프로그램으로서 동작하거나 대화형 셸을 사용하고 있는 경우는 셸이 드라이버 프로그램으로서 동작합니다.

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_6.png" width="700"/>

dirver program 내부에서 사용자가 처음으로 하는 것은 Spark Context를 생성하는 것이다. 예상하겠지만 Spark Context는 모든 spark 기능들을 접근하는 게이트웨이이다. spark context는 database connection과 비슷하다고 볼 수 있다. database 가 connection 되어 있다면 어느 command 명령어도 동작 시킬 수 있다. 마찬가지로, Spark context 를 통해서 어떤 spark 명령어도 사용할 수 있다.



이제 이 Spark context는 cluster manager 와 함께 여러 job들을 매니징한다. driver program 과 spark context는 cluster 에 있는 job execution들을 관리한다. job은 여러 task로 분할되고 이 task 들은 worker node에 분산된다. 어떤 때는 spark context에서는 RDD를 생성할 수 있다. 이 RDD는 여러 node로 분산되고 캐쉬될 수 있다.



worker node는 기본적으로 job을 task로 실행하는 slave node 이다. 이 task들은 worker node에서 파티션된 RDD에서 동작된다. 그리고 결과를 spark context로 넘겨준다.



Spark Context 는 job을 가지고, task에서 job을 멈추게 하고, worker node로 분산해준다. 이러한 task는 파티션된 RDD에서 일하고, 작업하고 결과를 취합하고 spark context로 반환한다.



만약 worker가 증가한다면, 사용자는 job을 더 많은 partition으로 나눌 수 있고 mulitple 시스템에서 별렬로 동작시킬 수 있다. 그럼 엄청 빨라지겠지!



worker 수의 증가함에 따라 memory size도 증가하여 job을 더 많이 캐쉬할 수 있고 따르게 동작 시킬 수 있다.

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_7.png" width="700"/>

- STEP 1 : 사용자는 코드를 spark 에 전송한다. 코드가 도착하면, driver는 뒤에서 우리의 코드의 transformations, action 작업을 DAG 로 바꾼다. (DAG : logically directed acyclic graph - 단방향을 가지는 비순환적인 그래프). 이 순서에서, 역시 파이프라인 변경과 같이 최적화해서 보여준다.




- STEP 2 : 그리고 나서, 논리적인 그래프인 DAG를 여러 단계(stage)의 물리적 실행 계획으로 이동시킨다. 물리적 실행 계획으로 변경시키고 나서는, 각각의 stage 에 물리적 실행 유닛인 task를 생성한다. 그리고 task는 묶여서 cluster 로 이동된다.



- STEP 3 : drvier는 cluster manager 와 자원에 대해 협상한다. cluster manager가 드라이버를 대신하여 worker node 에서 executor를 런칭한다. 이 시점에서 드라이버는 데이터 배치에 따라 task를 executor로 보낸다. executor가 시작될때, executor는 스스로 driver에 기록한다. 그래서 driver는 executor가 task를 동작 중인지 늘 명확히 확인할 수 있다.



- STEP 4 : execution의 task 코스가 진행되는 동안, driver program은 executor의 동작을 모니터링 한다. driver node 또한 데이터 배치에 따라 미래의 task를 스케쥴링한다.    

여기까지가 spark architecture 이다.


<br/><br/><br/>

## 4. Example using Scala in Spark shell


첫번째로, Hadoop 과 Spark daemon을 사용한다고 가정하고 spark shell을 시작한다. spark 의 Web UI 포트는 localhost:4040

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark 2_8.png" width="700"/>


word count 예제를 통해 어떻게 spark를 동작 시키는지 알아보자.
1. 이 예제에서, 간단한 text file 을 생성하고, hdfs 디렉터리에 저장했다. 다른 큰 데이터 파일도 가능하다.
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_9.png" width="700"/>   
2. spark shell을 start 했다면 RDD를 생성해보자. 여기서는, 특정한 Input file path 가지고, flatMap() 함수를 통해 transformation 한다.     

```
scala> var map = sc.textFile("hdfs://localhost:9000/Example/sample.txt").flatMap(line => line.split(" ")).map(word => (word,1)); 
```    
3. 이 코드를 실행시키면, RDD가 생성되어 보여진다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_10.png" width="700"/>   
4. 이후, RDD를 생성하기 위해 reduceByKey() action을 실행하자.    

```
scala> var counts = map.reduceByKey(_+_);
```
action을 실행하게 되면, execution이 실행되어 아래처럼 보여진다.     
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_11.png" width="700"/>   

5. 다음은 output 을 특정한 path에 text file 로 저장한다.     
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_12.png" width="700"/>   

6. 그리고 나서는 hdfs web browser (localhost:50040) 으로 접속한다. 그럼 output text 가 'part' 로 시작하는 파일명으로 저장된다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_13.png" width="700"/>   
7. 'part' 파일의 내용
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_14.png" width="700"/>    

spark application과 output 도출 방법에 대해 이해되었길 바란다.


<br/><br/>

이제, Spark 의 web UI를 사용해서 DAG를 시각화하고 execution task의 파티셔을 이해해보자.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_15.png" width="700"/>.   

- task를 클릭해 전송하면, DAG(단방향을 가지는 비순환적인 그래프) 의 완결된 job들을 볼 수 있다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_16.png" width="700"/>    

- 또한 작업 실행에 소요된 시간, 작업 ID, 완료된 단계, 호스트 IP 주소 등 실행된 작업의 요약 메트릭을 볼 수 있다.


<br/>


이제, 파티션과 RDD의 병렬화에 대해 이해해 보자.
- 파티션은 커다란 분배된 데이터 셋의 논리적인 덩어리이다.
- 기본적으로, spark는 가까운 노드에 있는 RDD를 읽으려고 한다.



이제, 어떻게 병렬 task 가 shell 에서 동작하는지 봐보자.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_17.png" width="700"/>.   

- 아래 그림은 RDD에서 총 파티션된 개수이다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_18.png" width="700"/>    

- 이제 5개의 다른 task들이 병렬로 동작하는 것을 볼 수 있다.    
<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark2_19.png" width="700"/>    

여기까지가 Apache Spark Architecture 설명이다.
