

출처 : https://spark.apache.org/docs/latest/rdd-programming-guide.html
참고 : https://helloino.tistory.com/20

<br/>

# 1. Overview

높은 수준에서, 모든 spark application은 driver program 으로 이루어져있다. driver program은 유저들의 메인 기능을 동작하고, 다양한 병렬 작업들을 클러스터에서 작업한다.    

spark의 추상적인 의미는 **탄력적인 분산 데이터셋 RDD를 제공한다는 것**이다. RDD는 클러스터의 노드들에서 파티션된 element의 모음이고, 병렬로 동작한다. 


RDD는 하둡 파일 시스템에서 파일들에 의해 생성되거나 driver program에서 scala 콜렉션에 의해 생성된 후 transform 된다. 사용자들은 아마 spark 가 메모리에 RDD로 계속 존재하도록 요청할 수도 있고, 병렬 작업을 효율적으로 다시 사용할 수 있도록 허용할 수도 있다.  RDD는 node 작업이 실패되면 자동을 복구도 된다.
<br/>

두번째 spark의 추상적인 컨셉은 **병렬 작업을 하기 위해 변수를 공유**하는 것이다. 기본적으로, spark 가 서로 다른 노드에 병렬 태스크로 동작하게 될 때, spark는 함수에 사용되는 각 변수의 복사본을 각각의 태스크에 보낸다. 가끔 변수들은 각각의 태스크에 공유되어야 하거나, 태스크와 driver program 사이에 공유되어야 한다. spark 는 두가지 타입의 공유 변수를 제공한다 : broadcast variables 와 accumulators. broadcast variable은 변수를 메모리의 모든 노드에 캐시로 사용되는 것이고, accumulator 는 counter 나 sum 같이 변수를 오직 더하기만 하는 것이다.

<br/><br/>

# 2. Resilient Distributed Datasets (RDDs)

spark 는 RDD(탄력적인 분산 데이터셋) 컨셉을 중심으로 전개된다.     
RDD는 병렬로 동작하는 element 들의 실패를 방지한다.     
RDD를 생성하는 두가지 방법이 있는데, driver program에서 이미 존재하는 **콜랙션을 병렬**로 배치하거나, 또는,  공유 파일시스템이나 HDFS, HBase 등등 Hadoop inputFormat 에서 제공하는 데이터 소스처럼 **외부 스토리지 시스템**에 데이터셋을 참조하는 것이다.    

<br/>

## 2-1. Parallelized Collections

병렬 콜렉션은 javaSparkContext 클래스에서 parallelize 메소드를 호출해서 driver program 에 이미 존재하는 콜렉션을 가지고 병렬 콜렉션을 생성한다. 콜렉션들은 병렬로 동작하는 분산 데이터셋을 복사한다. 아래 예시는 1부터 5까지 숫자를 어떻게 병렬 콜렉션으로 생성하는지에 대한 예시이다.

```java
List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD<Integer> distData = sc.parallelize(data);
```
한번 생성되면, 분산 데이터 셋은(distData) 병렬로 동작한다. 예를 들어 우리가 distData.reduce((a, b) -> a + b) 를 호출하면 이 메소드는 각 리스트의 element 를 합산한다. 분산 데이터셋을 동작 하는 방법은 추후 설명하겠다.    
병렬 콜렉션에서 중요한 파라미터는 데이터 셋을 자르는 파티션이다. Spark는 클러스터의 각 파티션에 대해 작업을 실행한다. 일단, 클러스터에서 한 cpu 당2~4개의 파티션이 생긴다. 일반적으로, spark는 클러스터를 기본으로 자동으로 파티션이 생성되지만, 다른 parallelize의 파라미터를 사용해 직접 세팅하기도 한다.    

<br/><br/>

## 2-2. External Datasets
spark는 분산 데이터셋을 생성할 수 있도록 하는 여러 Hadoop storage source 툴들을 제공하고 있다. 사용자의 로컬 file system , HDFS, cassandra, HBase, Amazon S3를 포함해서 기타 등등이 있다. spark는 text files, sequence file 과 기타 hadoop inputformat을 제공한다.    
text file RDD는 sparkContext 클래스의 textFile 메소드를 호출해 생성할 수 있다. 이 메소드는 로컬 파일의 path uri를 가지고 콜렉션 라인으로 읽는다.     
```java
JavaRDD<String> distFile = sc.textFile("data.txt");
```
한번 생성되면, distFile은 데이터셋 단위로 작동된다. map 과 reduce 작업 메소드를 사용해서 모든 라인을 합산하는 등의 작업을 할 수 있다. distFile.map(s -> s.length()).reduce((a, b) -> a + b).    

<br/>

참고 :    
local filesystem을 사용하면, 파일의 경로는 worker node에서도 동일하게 접근가능해야한다. 모든 worker 노드에서 파일을 복사하는 것 뿐만 아니라, file system을 네트워크에 마운트 하는 것 까지.    

spark의 input 메소드 요청에서는 파일은 디렉터리, 압축파일, wildcard 기능을 제공한다.    
textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").    

textFile은 추가적으로 파일의 파티션 크기를 설정할 수 있다. 기본적으로 spark에서 한 파티션은 각각의 file block 들을 생성하는데, 기본적으로 HDFS 에서는 block 당 128MB 이다. 더 크게 설정도 가능하다. 하지만 blocks 보다 더 적은 파티션은 불가하다는 것은 알아야 한다.    

text file 이외에도, spark 의 java api 역시 데이터 포맷을 제공한다.   

1. JavaSparkContext.wholeTextFiles :     
여러 개의 작은 텍스트 파일이 들어 있는 디렉토리를 읽고 각 파일을 (파일 이름, 내용) 쌍으로 반환. 이는 textFile이 각 파일의 한 줄당 하나의 레코드를 반환하는 것과는 대조적이다. 파티셔닝은 적은 파티션을 만들 수 있도록 데이터 지역성에 따라서 결정된다. 이렇게 파티셔닝을 조절하기 위한 매개 변수를 wholeTextFiles 함수는 두번째 인자에 옵션으로 제공하고 있다.
2. SparkContext.sequenceFile[K,V] :     
K 는 키, V 는 value. intWritable, Text 클래스 같이  Hadoop의 writable 인터페이스를 상속. Writable에 대한 기본 유형이 있는데, sequenceFile[Int, String] 로 하면 자동으로 IntWritables and Texts로 읽음.
3. JavaSparkContext.hadoopRDD :     
Hadoop inputFormat 하는 메소드.    
임의의 JobConf 및 입력 형식 클래스, 키 클래스 및 값 클래스를 사용. Hadoop 작업과 동일한 input source 방식.
4. JavaRDD.saveAsObjectFile , JavaSparkContext.objectFile :    
serialize 된 java 객체를 formatting 하여 RDD로 저장하는 기능. 

<br/><br/>

## 2-3. RDD Operations


RDD의 주된 역할은 transformation과 actions 이다.    
transformation은 기존 데이터 셋에서 새로운 데이터 셋을 생성하는 것이고,     
action은 데이터셋이 계산하고난 결과를 driver program으로 보내는 것이다.    

<br/>

예를 들어, map 은 transformation 기능인데, map 함수를 통해 각각의 데이터 셋 element 를 전달하고, 새로운 RDD를 결과로 반환한다.     
반면, reduce는 action 기능인데, 이 기능은 모든 RDD의 element 들의 함수의 결과를 취합하고 driver program으로 최종 결과를 반환한다.    

<br/>

모든 spark 의 transformation은 lazy 하다. 이 뜻은 transformation 은 계산을 하고 결과를 내는 걸 바로하지 않는다. 대신에, 단지 transformation이 데이터 셋을 변환할 거라는 것만 기억하기만 한다.        transformation은 action에서 결과를 요구할때만 작동하고 결과를 driver program으로 반환한다. 이 설계는 spark를 더 효과적으로 동작하게 한다. 예를들어, 우리는 map 을 이용해 데이터 셋을 생성할 때 reduce action이 일어날 것을 알고있다면 return은 거대한 데이터 셋을 mapping 할 필요 없이 그냥 reduce 의 결과만 driver 로 반환하면 된다. ( transformation의 결과를 반환할 필요가 없는 코드라면 매핑할 필요도 없는 작업이니까 동작을 하지 않는게 효과적이다. 자원이 배치된 배치될 상황을 미리 고려하여 최적의 코스를 돌 수 있다.).   

<br/><br/>

기본적으로 각각의 transformed RDD는 action 작업이 일어날때 재컴퓨트를 한다. 그러나 persist 나 cache 메소드를 사용해서 RDD를 메모리에 계속 유지할 수도 있다. 그럼 spark는 다음 쿼리를 접근할 때 element가 유지되고 있기 때문에 더 빠르게 접근할 수 있다. 디스크에서 RDD를 유지하거나 여러 노드에 걸쳐 복제하는 것도 지원된다.

<br/><br/>

## 2-4. Basics
RDD를 설명하자면..    
```java
JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
int totalLength = lineLengths.reduce((a, b) -> a + b);
```
첫줄은 외부파일로 부터 RDD를 정의하는 내용이다. action 할때까지 데이터 셋은 메모리에 올라가지 않는다. line 명령어는 단지 외부파일을 참조해놓은 것이다. lineLengths는 transformation을 하는 map 명령어이다. 다시말하지만 transfromation은 lazy 하기 때문에 lineLengths는 당장 바로 동작하지 않는다. 최종적으로 reduce를 action으로 동작한다. 여기서 spark는 분리된 여러 머신들에서 task를동작한다. 각각의 머신들은 map을 동작시키고 reduce하는 기능을 돌린며 결과를 driver program으로 반환한다.    

linelengths 를 추후에도 사용하고 싶다면 persist 를 사용하면 된다.    

```java
lineLengths.persist(StorageLevel.MEMORY_ONLY());
```
그럼, reduce 하기 전에 lineLength가 메모리로 저장될 수 있다.

## 2-5. Understanding closures
(참고) (Lexical) Closure. 클로저는 함수가 선언될 당시의 환경(environment) 을 기억했다가 나중에 호출되었을때 원래의 환경에 따라 수행되는 함수이다. 이름이 클로져인 이유는 함수 선언 시의 scope(lexical scope)를 포섭(closure)하여 실행될 때 이용하기 때문이다. 자주 '이름 없는 함수(익명함수)'와 혼동되곤 한다. 많은 언어의 익명함수가 closure를 포함하기 때문에 편하게 부를땐 서로 구분없이 부르기도 한다.    
현장에서 클로저가 자주 쓰이는 언어로는 JavaScript가 유명하다.    

<br/>

spark 에서 이해하기 어려운 부분은 클러스터에서 코드들이 동작할 때 변수나 메소드의 범위나 life cycle 에 대한 이해다. 범위를 벗어난 변수를 변경하는 RDD 작업은 자주 혼동을 준다. 아래 예시에서 foreach()를 사용해서 counter를 증가 시켜볼 것이다.    

아래 합산을 구하는 RDD를 보면, 동일한 JVM 에서도 execution의 동작에 따라 다르게 동작된다. spark가 local 모드일 때랑 클러스터에서 배포될 때를 비교해보자.    

```java
int counter = 0;
JavaRDD<Integer> rdd = sc.parallelize(data);
// Wrong: Don't do this!!
rdd.foreach(x -> counter += x);
println("Counter value: " + counter);
```

