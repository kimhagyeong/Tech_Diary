

출처 : https://spark.apache.org/docs/latest/rdd-programming-guide.html
참고 : https://helloino.tistory.com/20

<br/>

# 1. Overview

높은 수준에서, 모든 spark application은 driver program 으로 이루어져있다. driver program은 유저들의 메인 기능을 동작하고, 다양한 병렬 작업들을 클러스터에서 작업한다.    

spark의 추상적인 의미는 **탄력적인 분산 데이터셋 RDD를 제공한다는 것**이다. RDD는 클러스터의 노드들에서 파티션된 element의 모음이고, 병렬로 동작한다. 


RDD는 하둡 파일 시스템에서 파일들에 의해 생성되거나 driver program에서 scala 콜렉션에 의해 생성된 후 transform 된다. 사용자들은 spark 가 메모리에 RDD로 계속 존재하도록 요청할 수도 있고, 병렬 작업을 효율적으로 다시 사용할 수 있도록 허용할 수도 있다.  RDD는 node 작업이 실패되면 자동을 복구도 된다.
<br/>

두번째 spark의 추상적인 컨셉은 **병렬 작업을 하기 위해 변수를 공유**하는 것이다. 기본적으로, spark 가 서로 다른 노드에 병렬 태스크로 동작하게 될 때, spark는 함수에 사용되는 각 변수의 복사본을 각각의 태스크에 보낸다. 가끔 변수들은 각각의 태스크에 공유되어야 하거나, 태스크와 driver program 사이에 공유되어야 한다.    
spark 는 두가지 타입의 공유 변수를 제공한다 : broadcast variables 와 accumulators.    
broadcast variable은 변수를 메모리의 모든 노드에 캐시로 사용되는 것이고, accumulator 는 counter 나 sum 같이 변수를 오직 더하기만 하는 것이다.

<br/><br/>

# 2. Resilient Distributed Datasets (RDDs)

spark 는 RDD(탄력적인 분산 데이터셋) 컨셉을 중심으로 전개된다.     
RDD는 병렬로 동작하는 element 들의 실패를 방지한다.     
RDD를 생성하는 두가지 방법이 있는데, driver program에서 이미 존재하는 **콜랙션을 병렬**로 배치하거나, 또는,  공유 파일시스템이나 HDFS, HBase 등등 Hadoop inputFormat 에서 제공하는 데이터 소스처럼 **외부 스토리지 시스템**에 데이터셋을 참조하는 것이다.    

<br/>

## 2-1. Parallelized Collections

병렬 콜렉션은 javaSparkContext 클래스에서 parallelize 메소드를 호출해서 driver program 에 이미 존재하는 콜렉션을 가지고 병렬 콜렉션을 생성한다. 콜렉션들은 병렬로 동작하는 분산 데이터셋을 복사한다. 아래 예시는 1부터 5까지 숫자를 어떻게 병렬 콜렉션으로 생성하는지에 대한 예시이다.

```java
List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD<Integer> distData = sc.parallelize(data);
```
한번 생성되면, 분산 데이터 셋은(distData) 병렬로 동작한다. 예를 들어 우리가 distData.reduce((a, b) -> a + b) 를 호출하면 이 메소드는 각 리스트의 element 를 합산한다. 분산 데이터셋을 동작 하는 방법은 추후 설명하겠다.    
병렬 콜렉션에서 중요한 파라미터는 데이터 셋을 자르는 파티션이다. Spark는 클러스터의 각 파티션에 대해 작업을 실행한다. 일단, 클러스터에서 한 cpu 당2~4개의 파티션이 생긴다. 일반적으로, spark는 클러스터를 기본으로 자동으로 파티션이 생성되지만, 다른 parallelize의 파라미터를 사용해 직접 세팅하기도 한다.    

<br/>

팁 : 코드에서 term slices 는 partitions 과 동음이어로 쓰이고 뒷단에서 호환가능하게 유지할 수 있다.

<br/><br/>
## 2-2. External Datasets
spark는 분산 데이터셋을 생성할 수 있도록 하는 여러 Hadoop storage source 툴들을 제공하고 있다. 사용자의 로컬 file system , HDFS, cassandra, HBase, Amazon S3를 포함해서 기타 등등이 있다. spark는 text files, sequence file 과 기타 hadoop inputformat을 제공한다.    

text file RDD는 sparkContext 클래스의 textFile 메소드를 호출해 생성할 수 있다. 이 메소드는 로컬 파일의 path uri를 가지고 콜랙션 라인으로 읽는다.     
```java
JavaRDD<String> distFile = sc.textFile("data.txt");
```
한번 생성되면, distFile은 데이터셋 단위로 작동된다. map 과 reduce 작업 메소드를 사용해서 모든 라인을 합산하는 등의 작업을 할 수 있다. distFile.map(s -> s.length()).reduce((a, b) -> a + b).    

<br/>

참고 :    
local filesystem을 사용하면, 파일의 경로는 worker node에서도 동일하게 접근가능해야한다. 모든 worker 노드에서 파일을 복사하는 것 뿐만 아니라, file system을 네트워크에 마운트 하는 것 까지.    

spark의 input 메소드 요청에서는 파일은 디렉터리, 압축파일, wildcard 기능을 제공한다.    
textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").    

textFile은 추가적으로 파일의 파티션 크기를 설정할 수 있다. 기본적으로 spark에서 한 파티션은 각각의 file block 들을 생성하는데, 기본적으로 HDFS 에서는 block 당 128MB 이다. 더 크게 설정도 가능하다. 하지만 blocks 보다 더 적은 파티션은 불가하다는 것은 알아야 한다.    

text file 이외에도, spark 의 java api 역시 데이터 포맷을 제공한다.   

1. JavaSparkContext.wholeTextFiles :     
여러 개의 작은 텍스트 파일이 들어 있는 디렉토리를 읽고 각 파일을 (파일 이름, 내용) 쌍으로 반환. 이는 textFile이 각 파일의 한 줄당 하나의 레코드를 반환하는 것과는 대조적이다. 파티셔닝은 적은 파티션을 만들 수 있도록 데이터 지역성에 따라서 결정된다. 이렇게 파티셔닝을 조절하기 위한 매개 변수를 wholeTextFiles 함수는 두번째 인자에 옵션으로 제공하고 있다.
2. SparkContext.sequenceFile[K,V] :     
K 는 키, V 는 value. intWritable, Text 클래스 같이  Hadoop의 writable 인터페이스를 상속. Writable에 대한 기본 유형이 있는데, sequenceFile[Int, String] 로 하면 자동으로 IntWritables and Texts로 읽음.
3. JavaSparkContext.hadoopRDD :     
Hadoop inputFormat 하는 메소드.    
임의의 JobConf 및 입력 형식 클래스, 키 클래스 및 값 클래스를 사용. Hadoop 작업과 동일한 input source 방식.
4. JavaRDD.saveAsObjectFile , JavaSparkContext.objectFile :    
serialize 된 java 객체를 formatting 하여 RDD로 저장하는 기능.  반면에 이건 Avro와 같은 특정한 포맷에 있어서는 효과적이지는 않지만, RDD를 저장할때는 쉬운길을 제공한다.


<br/><br/>

## 2-3. RDD Operations


RDD의 주된 역할은 transformation과 actions 이다.    
transformation은 기존 데이터 셋에서 새로운 데이터 셋을 생성하는 것이고,     
action은 데이터셋이 계산하고난 결과를 driver program으로 보내는 것이다.    

<br/>

예를 들어, map 은 transformation 기능인데, map 함수를 통해 각각의 데이터 셋 element 를 전달하고, 새로운 RDD를 결과로 반환한다.     
반면, reduce는 action 기능인데, 이 기능은 모든 RDD의 element 들의 함수의 결과를 취합하고 driver program으로 최종 결과를 반환한다.    

<br/>

모든 spark 의 transformation은 lazy 하다. 이 뜻은 transformation 은 계산을 하고 결과를 내는 걸 바로하지 않는다. 대신에, 단지 transformation이 데이터 셋을 변환할 거라는 것만 기억하기만 한다.        transformation은 action에서 결과를 요구할때만 작동하고 결과를 driver program으로 반환한다. 이 설계는 spark를 더 효과적으로 동작하게 한다. 예를들어, 우리는 map 을 이용해 데이터 셋을 생성할 때 reduce action이 일어날 것을 알고있다면 return은 거대한 데이터 셋을 mapping 할 필요 없이 그냥 reduce 의 결과만 driver 로 반환하면 된다.    

-> transformation의 결과를 반환할 필요가 없는 코드라면 매핑할 필요도 없는 작업이니까 동작을 하지 않는게 효과적이다. 자원이 배치된 배치될 상황을 미리 고려하여 최적의 코스를 돌 수 있다.    

<br/><br/>

기본적으로 각각의 transformed RDD는 action 작업이 일어날때 재컴퓨트를 한다. 그러나 persist 나 cache 메소드를 사용해서 RDD를 메모리에 계속 유지할 수도 있다. 그럼 spark는 다음 쿼리를 접근할 때 element가 유지되고 있기 때문에 더 빠르게 접근할 수 있다. 디스크에서 RDD를 유지하거나 여러 노드에 걸쳐 복제하는 것도 지원된다.

<br/><br/>

## 2-4. Basics
RDD를 설명하자면..    
```java
JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
int totalLength = lineLengths.reduce((a, b) -> a + b);
```
첫줄은 외부파일로 부터 RDD를 정의하는 내용이다. action 할때까지 데이터 셋은 메모리에 올라가지 않는다. line 명령어는 단지 외부파일을 참조해놓은 것이다. lineLengths는 transformation을 하는 map 명령어이다. 다시말하지만 transfromation은 lazy 하기 때문에 lineLengths는 당장 바로 동작하지 않는다. 최종적으로 reduce를 action으로 동작한다. 여기서 spark는 분리된 여러 머신들에서 task를동작한다. 각각의 머신들은 map을 동작시키고 reduce하는 기능을 돌린며 결과를 driver program으로 반환한다.    

linelengths 를 추후에도 사용하고 싶다면 persist 를 사용하면 된다.    

```java
lineLengths.persist(StorageLevel.MEMORY_ONLY());
```
그럼, reduce 하기 전에 lineLength가 메모리로 저장될 수 있다.

## 2-5. Understanding closures
(참고) (Lexical) Closure. 클로저는 함수가 선언될 당시의 환경(environment) 을 기억했다가 나중에 호출되었을때 원래의 환경에 따라 수행되는 함수이다. 이름이 클로져인 이유는 함수 선언 시의 scope(lexical scope)를 포섭(closure)하여 실행될 때 이용하기 때문이다. 자주 '이름 없는 함수(익명함수)'와 혼동되곤 한다. 많은 언어의 익명함수가 closure를 포함하기 때문에 편하게 부를땐 서로 구분없이 부르기도 한다.    
현장에서 클로저가 자주 쓰이는 언어로는 JavaScript가 유명하다.    

<br/>

spark 에서 이해하기 어려운 부분은 클러스터에서 코드들이 동작할 때 변수나 메소드의 범위나 life cycle 에 대한 이해다. 범위를 벗어난 변수를 변경하는 RDD 작업은 자주 혼동을 준다. 아래 예시에서 foreach()를 사용해서 counter를 증가 시켜볼 것이다.    

아래 합산을 구하는 RDD를 보면, 동일한 JVM 에서도 execution의 동작에 따라 다르게 동작된다. spark가 local 모드일 때랑 클러스터에서 배포될 때를 비교해보자.    

```java
int counter = 0;
JavaRDD<Integer> rdd = sc.parallelize(data);
// Wrong: Don't do this!!
rdd.foreach(x -> counter += x);
println("Counter value: " + counter);
```

<br/><br/>

### Local vs cluster modes
위에 있는 코드는 동작은 정의되지 않았으며 의도한 대로 작동하지 않을 수 있다. job을 동작시키려면 spark는 RDD operation을 테스크로 분해하고, 각각의 task는 executor에서 동작된다. execution 동작 전에 spark는 task의 closure를 작업한다. closure는 executor가 RDD에서 계산을 위해 보여야 하는 변수나 메서드로 (여기서는 foreach()) closure은 serialize 하고, 각각의 executor로 작업으로 보낸다.     

<br/>

각 executor에게 전송되는 closure 내의 변수는 이제 복사본이므로 foreach 함수에서 counter가 참조되면 더 이상 드라이버 노드의 counter 가 아니다. 드라이버 노드의 메모리에서 counter가 여전히 있어도, executor 에서는 보이지 않고 복사된 closure 변수만 있다. 그러므로 counter의 모든 연산이 serialized closure 내의 값을 참조하고 있었기 때문에 카운터의 최종 값은 0.    

-> foreach 함수에서 참조하는 counter 는 드라이버 노드의 counter 가 아니라 복사본이기 때문에 foreach 가 끝나도 counter 는 0이다.     

<br/>

foreach 에서 counter도 계속 업데이트를 하고 싶다면, Accumulator를 사용해보자. spark에서 Accumulator 는 cluster 워커 노드에서 작업이 분리되었을 때 안전하게 변수를 업데이트하고자 할 때 사용한다. 변수를 global 하게 사용하고 싶을 때 accumulator를 사용한다.


### printing elements of an RDD

일반적으로 RDD.foreach(println), RDD.map(println) 사용해서 출력하는 시도를 하고 싶겠지만, 이 뜻은 모든 RDD elements 에서 출력을 시도한다는 것이다. 클러스터 모드에서 executor에서 stdout 를 출력하는 것은 driver에서 출력하는게 아니기 때문에 보여지지 않는다. 모든 elements를 driver로 출력하고 싶다면 collect() 메소드를 사용해서 첫번째 RDD를 driver 노드에 불러야 한다. : RDD.collect().foreach(println)     

collect()가 모든 RDD를 단일 머신으로 패치되기 때문에 drvier로 출력 가능하다. 일부 RDD 요소만 출력하고 싶다면 take() 을 쓰자. RDD.take(100).foreach(println)    

-> executor element 에서 println, stdout 같이 로그 출력이 불가능하다. (출력은 drvier단에서 함)  병렬 분산된 실행자에서 element 들을 출력하려면 단일 머신에서 돌려야 하므로 collect, take 메소드를 사용해서 단일 노드로 만들고 출력하자.   

<br/><br/>

# 3. Working with Key-Value Pairs

대부분의 spark operation은 RDD 객체 타입에서 동작하는데, 특정 operations 들은 key value 형태의 RDD 에서만 동작한다. 그에 대한 예로 분산 작업인 “shuffle” 이 있는데, 이건 key 를 통해 그룹핑하거나 취합하는 것이다.    

Java 에서 key-value pairs operation 은 scala.Tuple2 에서 동작되는데, 간단히 new Tuple2(a,b)를 불러서 튜플을 생성하고, tuple._1(), tuple._2() 를 사용해서 필드에 접근한다.    

key-value pairs RDD는 JavaPairRDD 클래스를 통해 사용된다. JavaPairRDD는 JavaRDD를 상속해서 만들고, 새로운 map operation 인 mapToPair, flatMapToPair operation 이 추가된다. JavaPairRDD는 key-value를 지원하는 기본적인 RDD 기능이다.    

예) reduceByKey 를 사용해서 파일에 텍스트 라인이 몇 개 있는지 세는 작업.   

```java
JavaRDD<String> lines = sc.textFile("data.txt");
JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));
JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);
```
각 쌍을 알파벳 배열로 나열할 때 counts.sortBykey() 를 사용하고, array 객체로 driver program으로 반환할때 counts.collect() 를 사용한다.    
<br/>
참고: key-value pair 작업에서 사용자 지정 객체를 키로 사용할 때 사용자 지정 equals() 메서드가 hashCode() 메서드와 함께 제공되는지 확인해야 한다. 자세한 내용은 Object.hashCode() 문서.    
-> 동일한 객체는 동일한 메모리 주소를 갖는다는 것을 의미하므로, 동일한 객체는 동일한 해시코드를 가져야 한다. 그렇기 때문에 만약 우리가 equals() 메소드를 오버라이드 한다면, hashCode() 메소드도 오버라이드 되어야 한다. 

<br/><br/>
# 4. Transform RDD
* https://helloino.tistory.com/20 에서 가져옴    
|Transformation|의미|
|----|----------------|
|map(func)|새로운 타입의 RDD를 결과값을 얻는다.|
|filter(func)|조건에 필터된 RDD를 얻는다.|
|flatMap(func)|map와 비슷하지만 input값이 0개로 맵핑 될수도 있고 n개로 맵핑이 될수가 있다.|
|mapPartitions(func)| map(func)와 비슷하지만 partition 단위 map 연산을 한다. 따라서 Iterator 객가 input/ouput값이다. (Iterator<T>)=>Iterator<U>|
|mapPartitionsWithIndex(func)|mapPartitions가 동일하나 index가 존재한다 (i, Iterator<T>)=>Iterator<U>|
|sample(withReplacement, fraction, seed)|샘플링된 RDD로 리턴한다.|
|union(otherDataset)|다른 RDD와 합쳐진 결과값을 리턴한다.|
|intersection(otherDataset)|2개의 RDD에서 서로 교차 되어 있는 (intersection) 값 리턴한다.|
|distinct([numTasks]))|중복된 값 제거, SQL distinct와 유사하다.|
|groupByKey([numTasks])|(K, V) Pair 객체를 이용하여 (K, Iterator<V>) 만들어 준다. sql의 group by 기능과 동일하다.|
|reduceByKey(func, [numTasks])|(K, V) Pair 객체에서 Key 기준으로 reduce 할때 사용한다.|
|aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])|(K,V) 객체에서 (K, U) 객체로 aggregate|
|sortByKey([ascending], [numTasks])|Key 값을 기준으로 sort (desc, asc)|
|join(otherDataset, [numTasks])|(K, V) RDD와 (K, U) RDD을 join (K, (V, U)) 값을 리턴한다.|
|cogroup(otherDataset, [numTasks])|(K, V) RDD와 (K,U) RDD를 join (K (Iterator <V>, Iterator<U>))|
|pipe(command, [envVars])|shell command를 통해 rdd를 생성할 수 있다. shell command의 pipe 기능과 유사 (>, >>, >>>)|
|coalesce(numPartitions)|partition 갯수를 줄인다.|
|repartition(numPartitions)|partition을 다시 shuffle 한다. partition의 갯수를 늘리거나 줄일수가 있다.|
|repartitionAndSortWithinPartitions(partitioner)|RDD의 partition를 다시 진행한다. 각각의 partion은 sort가 된 값들이 존재한다.|
  
<br/><br/>

  

# 5. Action RDD
|Action|의미|
|----|----------------|
|reduce(func)	|데이터셋의 데이터를 하나로 만들어 준다(aggregate)|
|collect()|	모든 RDD를 driver 프로그램에 array 형태로 리턴한다.|
|count()|	데이터의 갯수를 리턴한다.|
|first()|	첫번째 데이터를 가지고 온다.|
|take(n)|	n개의 데이터를 가지고 온다.|
|takeSample(withReplacement, num, [seed])|	num 갯수의 데이터를 샘플링한다.|
|takeOrdered(n, [ordering])|	sort된 데이터 중에 n개를 가지고 온다.|
|saveAsTextFile(path)|	데이터를 파일로 저장한다.|
|saveAsSequenceFile(path)|	데이터를 SequenceFile로 저장한다.|
|saveAsObjectFile(path)|	데이터를 ObjectFile로 저장한다.|
|countByKey()|	(K,V) Pair 객체일때만 사용이 가능한다. 각각의 key에 대해 갯수를 센다 (K, int)|
|foreach(func)|	각각의 데이터에 대해 함수를 실행시킨다 (for-문)|
    
Spark RDD API 는 일부 비동기 action 을 제공하는데, foreach 에서 foreachAsync 서비스와 같이 작업 완료 시 작업을 종료하기 보다 호출자에게 FutureAction을 반환한다. FutureAction은 java에서 익숙하면 알다시피 멀티 프로세스로 작업을 요청하고 그 결과를 추후에 취합하겠다는 의미로 쓰이는데, 이것도 비동기의 일종임을 눈치챌 수 있다.    

  <br/><br/>

# 6. Shuffle operations
  
Spark event 트리거에서의 작업은 shuffle이라고 불린다. shuffle은 파티션간 서로 다르게 그룹하하여 spark의 머신을 재분배하는 매커니즘이다. shuffle은 executor 와 머신간에 데이터를 복사하는 작업이 포함되어, shuffle을 복잡하고 비용이 드는 작업으로 만든다.    
  
<br/><br/>

## 6-1. Background
shuffle을 이해하기 위해, reduceByKey operation에 대해 이해해야한다. reduceByKey operation은 single key 에 대한 모든 값을 tuple로 결합되는 새 RDD를 생성한다. 각각의 operation 은 꼭 같은 파티션이나 머신에 있다는 보장이 없고 결과는 같은 위치에서 내야 하기 때문에 새 RDD를 생성한다.    
-> RDD 에서 RDD 만들고 또 RDD 만들어서 RDD lineage를 만든다    

  <br/>

reduceByKey 는 동일한 Key 를 가지고 있는, 모든 record 값을 취합하는 작업. (A, 1), (A, 2), (A, 3) → (A, 6)    
하지만, Spark 의 분산처리는 파티션 단위로 진행되기 때문에,동일한 Key 의 모든 record 값을 취합하기 위해선, 동일한 Key 를 가진 튜플 데이터가 전부 같은 파티션에 있어야 함.    
따라서, 모든 튜플 데이터가 여러 클러스터에 분산 저장되어 있을 때, 동일한 Key 를 가진 튜플 데이터를 동일한 파티션에 두기 위해, 데이터의 위치를 재조정하는 방법이 Shuffle.    
- repartition operations (repartition, coalesce)
- ByKey operations (groupByKey, reduceByKey)
- join operations (cogroup, join)

<br/><br/>

## 6-2. Performance Impact

shuffle은 disk I/O, 데이터 직렬화(serialization), 네트워크 I/O를 해야하기에 비싼 작업이다. shuffle을 하기 위해서 spark는 task를 만드는데, map 은 data를 organize 하는 작업이고, reduce 는 취합하는 작업이다. 이름이 map, reduce여서 MapReduce 작업이랑 비슷해보이지만 spark에서 map, reduce 작업은 다르다.    

내부적으로 각각의 map task 결과는 메모리에 건들 수 없게 존재하고, 타겟 파티션을 기반으로 sort 하고 하나의 파일에 write 한다. reduce에서는 sort된 block 을 읽는다.    
  
특정 shuffle operation은 메모리 데이터 구조상 transferring 전후로 기록해야하기 때문에 heap memory 가 상당히 소비된다. 특히 reduceByKey 와 aggregateByKey는 map작업과, ‘ByKey’ 라는 reduce 의미의 작업이라는 의미가 있다. 데이터가 메모리에 맞지 않을 때 Spark는 테이블을 디스크로 옮기게 되고, 그럼 disk I/O overhead와 garbage collection을 발생시킨다    
  
<br/>

shuffle은 디스크에 많은 중간 파일을 생성한다. Spark 1.3부터 이러한 중간 파일은 RDD가 사용되지 않고 garbage collection이 될 때까지 보존된다. 이렇게 보존되면  프로그램이 재 시작되고 re-computed가 되어도 shuffle file을 생성하지 않는다. 그럼 garbage collection은 언제 일어나냐면, 어플리케이션에 RDD가 오래 유지되어서 GC 가 자주 시작되지 않는 경우, 즉 시간이 오래 걸리면 일어난다. 이는 long-running spark jobs은 많은 양의 디스크 공간을 사용할 수 있음을 의미한다. 임시 저장소 디렉토리(중간 파일)는 Spark 컨텍스트를 구성할 때 spark.local.dir 구성 매개변수에 의해 지정됩니다. shuffle 동작은 config 할 때 파라미터로 조절할 수 있다.    
-> 그래서 프로그램이 강제 종료되는 경우, tmp 디렉터리에서 중간 파일을 삭제하고 재시작하는 메뉴얼이 있다.
  
  
<br/><br/>

# 7. RDD Persistence

spark 에서 가장 중요한 기능은 데이터 셋을 각각의 operation 에 보관(캐싱) 하는 것이다. RDD를 저장할 때 각 노드는 메모리를 계산하는 파티션을 저장하고, action을 하는 데이터 셋(혹은 이에 파생된 데이터 셋)에서 재사용한다. 이런 future action은 처리 속도를 10배 넘게 빠르게 한다. 캐싱은 반복 알고리즘과 빠른 대화형 사용을 위한 핵심 도구이다.    

메소드 : persist(), cache()
처음에는 action으로 계산하고, 노드에 메모리로 유지한다. spark 캐시는 fault-tolerant 이다. 만약 RDD 파티션이 누락돼도, 자동적으로 파티션을 생성했던 transformation을 사용해 recomputed 된다.    


덧붙여서, 각 persisted RDD는 서로다른 storage level을 사용해 저장되고 허용한다. 예를들어, dataset을 디스크에 persist 하면 메모리에 유지되지만 직렬화된 Java 객체로(공간 절약을 위해) 노드 간에 복제한다.     
이러한 level은 StorageLevel 객체(Scala, Java, Python)를 persist()에 전달하여 설정된다. cache() 메서드는 StorageLevel.MEMORY_ONLY(deserialize 된 개체를 메모리에 저장하는 기능)인 기본 storage level을 사용하기 위한 축약형이다.

  <br/>
  
|Storage Level|의미|
|---|------|
|MEMORY_ONLY|JVM에 자바 객체(deserialized)를 메모리에서만 저장, 메모리가 부족하다면 저장되지 않는다. 만약 다시 필요하다면 다시 계산을 한다.|
|MEMORY_AND_DISK|JVM에서 자바 객체(deserialized)를 메모리에서 저장, 메모리가 부족하다면 디스크에 저장한다. 용량도 부족하다|
|MEMORY_ONLY_SER (Java and Scala)|자바 객체 (serialized) 객체를 메모리에서만 저장, fast serialization library사용을 할수가 있으나 이럴 경우에는 cpu 사용량이 높아진다.|
|MEMORY_AND_DISK_SER (Java and Scala)|자바 객체 (serialized) 객체를 메모리, 디스크에 저장|
|DISK_ONLY|DISK에서만 저장을 한다.|
|MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.|다른 기능들과 동일하나, 2개의 cluster node에 대해 복제 (replicated) 저장을 한다.|
|OFF_HEAP (experimental)|MEMORY_ONLY_SER 비슷하나 off-heap memory에 저장이 된다.|
  
Python이 인경우에는 Pickle를 통해 항상 직렬화를 하기 때문에 serialized level이 존재하지 않는다. python 에서 사용되는 storage level 은 MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, DISK_ONLY_2, and DISK_ONLY_3.    
  
<br/>

또한 spark는 사용자가 persist를 호출하지 않아도 셔플 작업( 예 : reduceByKey) 에서 자동으로 중간 데이터를 persist 한다. 이렇게 중간 데이터를 persist 하는 것은 shuffle 중에 node 가 fail 되도 전체 input 을 recomputing 하는 것을 피하기 위해 수행된다. 우리는 user 가 reuse 를 사용할 계획이 있다면 RDD 결과를 persist 하기를 권한다.     

  
<br/><br/>

## 7-1. Which Storage Level to Choose?
  
spark storage level을 사용하는 이유는 메모리 사용량과 cpu 효율성을 고려해 서로 다른 균형을 제공하기 위해서 이다. 다음 프로세스를 통해 한가지 선택하는 게 좋다.    
  
- 만약 RDD가 기본저장수준 (MEMORY_ONLU)에 완전히 적합하다면 그냥 둬라. 그건 가장 호율적으로 CPU를 사용하는 옵션이고, RDD에서 가장 빠르게 실행할 수 있는 옵션이다.
- 아니라면 MEMORY_ONLY_SER를 사용하고 빠르게 serialization을 하는 라이브러리를 선택해서 객체를 훨씬 공간 효율적으로 만들게 한다. 그럼에도 여전히 합리적으로 빠르다.
- 데이터 세트를 계산하는 함수가 비싸거나 많은 양의 데이터를 필터링하지 않는 한 디스크에 유출하지 않도록 해라. 그렇지 않으면 파티션을 다시 계산하는 것이 디스크에서 읽는 것만큼 빠를 수 있다.
- 빠른 실패 복구를 위해서는 복제된 storage level 를 사용해라. (예를 들어 spark 가 웹에서 request 서비스를 제공할 때). 모든 storage level은 손실된 데이터를 다시 계산하여 완전한 fault tolerance를 제공하지만 복제된 저장소 수준을 사용하면 손실된 파티션을 다시 계산할 때까지 기다리지 않고도 RDD에서 작업을 계속 실행할 수 있다.
  
<br/><br/>
  
## 7-2. Removing Data
Spark는 각 노드의 캐시 사용량을 자동으로 모니터링하고 LRU(least-recently-used ) 방식으로 오래된 데이터 파티션을 삭제. 메뉴얼하게 RDD를 지우려면 캐시가 비워질때까지 기다리지 말고 RDD.unpersist() 메소드를 사용. 이 메소드는 default 로 막혀있지 않다. 리소스가 해제될때까지 차단하려면, 메소드를 호출할 때 blocking=true를 하면 된다.
  
  
# 8. Shared Variables
map, reduce 와 같은 execute operation 에서는 변수를 공유하지 않는다. 내부적으로 아무리 변경해도 적용되지 않음. (해봄.. 이건 spark 가 아닌 병렬처리에서도 동일..)  이걸 허용하도록 공유 변수를 제공하는데 broadcast 와 accumulator
  
<br/>

## 8-1. Broadcast Variables
broadcast 변수를 사용하면 task operation 복사본을 배송하는 대신 각 시스템에 캐시된 읽기 전용 변수를 각 머신에 유지할 수 있다. 어떻게 사용되냐면, 예를 들어, 효율적인 방식으로 large input dataset 복사본을 모든 노드에 제공할 수 있다.    
spark는 비용을 줄이기 위해 효율적인 broadcast 알고리즘을 사용하여 broadcast 변수를 분배한다. spark는 분산된 shuffle operation으로 세팅된 스테이지를 통해 실행된다. spark는 자동으로 각 스테이지에 공통된 데이터를 자동으로 broadcast 한다. broadcast 된 데이터는 task 가 실행되기 전에 serialize 된 형식으로 캐시되고 역직렬화된다. 즉, 여러 단계에 걸친 작업이 동일한 데이터를 필요로 하거나 역직렬화된 형식으로 데이터를 캐싱하는 것이 중요한 경우에 유용하다.    
공유 변수를 SparkContext.broadcast (v) 로 싸고, value 메소드를 통해 접근한다.

```java
Broadcast<int[]> broadcastVar = SparkContext.broadcast(new int[] {1, 2, 3});
broadcastVar.value();
```

모든 노드에서 동일한 값을 보장하기 위해 한번 생성된 이유로 수정이 되지 않는다. broadcast 변수의 리소스를 해제하려면 .unpersist() 를 호출한다. 만약 broadcast 다시 사용되면 재방송됨. 영구 해제는 .destory() . 참고로 boradcast 변수는 차단이 default 가 아니여서 리소스가 해제될때까지 차단하려면 blocking=true 지정 필요.    
  
<br/><br/>
  
## 8-2. Accumulators
accumulator는 “add” 만 지원해서 효율적으로 병렬이 지원된다. accumulator는 counter(mapreduce) 나 sum 이 내장되어 있다.     
숫자 누산기는 각각 Long 또는 Double 유형의 값을 누적하기 위해 SparkContext.longAccumulator() 또는 SparkContext.doubleAccumulator()를 호출하여 생성할 수 있다.클러스터에서 실행 중인 작업은 add 메서드를 사용하여 클러스터에 추가할 수 있으나 그들은 그 값을 읽을 수 없다. 드라이버 프로그램만 값 메서드를 사용하여 누산기의 값을 읽을 수 있다.

```java
LongAccumulator accum = jsc.sc().longAccumulator();

sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -> accum.add(x));
// ...
// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

accum.value();
// returns 10

```
AccumulatorV2 추상 클래스는 는 accumulator를 0으로 세팅하는 rest 기능을 override 한다.
```java
  class VectorAccumulatorV2 implements AccumulatorV2<MyVector, MyVector> {

  private MyVector myVector = MyVector.createZeroVector();

  public void reset() {
    myVector.reset();
  }

  public void add(MyVector v) {
    myVector.add(v);
  }
  ...
}

// Then, create an Accumulator of this type:
VectorAccumulatorV2 myVectorAcc = new VectorAccumulatorV2();
// Then, register it into spark context:
jsc.sc().register(myVectorAcc, "MyVectorAcc1");
```
경고: Spark 작업이 완료되면 Spark는 이 작업의 누적된 업데이트를 accumulator에 병합하려고 시도한다. 실패하면 Spark는 실패를 무시하고 여전히 작업을 성공한 것으로 표시하고 다른 작업을 계속 실행한다. 따라서 accumulator에 버그가 있어도 spark 작업엔 영향이 없지만, 결과가 언제나 올바르게 업데이트되지 않을 수 있다.    
spark accumulator 업데이트는 actions only 상황일 때 각 task는 일어날때 한번만 업데이트되도록 보장하기에, 다시 시작된 작업은 값을 업데이트하지 않는다. transformation 상황일 때는 각 task가 두번이상 업데이트 될수도 있음.    
accumulator는 tansformation (map같은) 도중에 업데이트 되는 게 실행이 보장되지 않는다. RDD 도중에 일어나지 않음.     

```java
LongAccumulator accum = jsc.sc().longAccumulator();
data.map(x -> { accum.add(x); return f(x); });
// Here, accum is still 0 because no actions have caused the `map` to be computed.
```

  
