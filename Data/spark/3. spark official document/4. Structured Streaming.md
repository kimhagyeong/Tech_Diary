# Structured Streaming Programming Guide
doc : https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html   

# 1. Overview

구조적 스트리밍은 Spark SQL 엔진을 기반으로 구축된 확장 가능하고 fault-tolerance가 있는 스트림 처리 엔진이다. 정적 데이터에 대한 일괄 계산을 표현하는 것과 같은 방식으로 스트리밍 계산을 표현할 수 있다.   
시스템은 체크포인트 및 로그를 통해 종단 간 fault-tolerance를 보장함. 간단히 말해서, 구조적 스트리밍은 사용자가 스트리밍에 대해 생각할 필요 없이 빠르고 확장 가능하며 fault-folerant하고, end-to-end 에서 정확히 1회 스트림 처리를 제공.    
내부적으로, structured streaming 쿼리는 기본적으로 마이크로배치 처리 엔진을 사용하여 처리된다. micro batch processing은 데이터 스트림을 일련의 작은 배치 작업으로 처리하여 100밀리초이내에 end-to-end 의 대기시간과 정확히 한 번의 fault-tolerant를 보장한다. spark 2.3에서는 새로운 저지연 프로세싱 모델인 continuous processing 을 제공하는데, 이 프로세싱은 1밀리초만큼 낮은 end-to-end 대기 시간과 최소 1회 fault-tolerant 보장한다. 쿼리에서 dataset/dataframe 작업 변경 없이 어플리케이션 요구 사항에 따라 모드를 바꿀 수 있다.    
이 가이드에서는 micro-batch 프로그래밍 모델과 API를 안내한다.    

# 2. Quick Example

```
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;

import java.util.Arrays;
import java.util.Iterator;

SparkSession spark = SparkSession
  .builder()
  .appName("JavaStructuredNetworkWordCount")
  .getOrCreate();
// Create DataFrame representing the stream of input lines from connection to localhost:9999
val lines = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

// Split the lines into words
val words = lines.as[String].flatMap(_.split(" "))

// Generate running word count
val wordCounts = words.groupBy("value").count()
```
여기서 lines dataframe은 streaming text data를 포함하는 한정없는 데이블을 의미한다. 이 테이블은 value라는 스트링 이름의 칼럼을 가지고 있고, 스트리밍에서 각 라인들은 테이블의 row가 된다. 눈여겨볼점은, 이 테이블은 트랜스포메이션 세팅만 해놨지 시작하지도, 데이터를 가져오지도 않았다. 다음으로, 우리는 dataframe을 dataset으로 .as 를 사용해서 변경하여, flatMap 오퍼레이션을 통해 다중 단어들로 분리할 수 있다. 결론적으로 words 데이터셋에는 모든 단어가 포함되어 있다. 최종적으로, wordcounts 데이터프레임은 dataset의 고유한 값으로 그룹화하고 계산함으로 정의된다. 이게 바로 running word count stream을 나타내는 streaming dataframe이다.
우리는 이제 streaming data 에 대한 query 를 세팅했다. 이제 남은건 실제 데이터를 받아오고 counts를 계산하는 것이다. 이걸 하기 위해서 우리는 콘솔에 업데이트를 확인하기 위해 완전한 count 셋을 프린트해야한다. 그리고 streaming 컴퓨팅을 start()를 통해 시작한다.


<br/><br/>

# 3. Programming Model

structured streaming 의 핵심 아이디어는 live data stream을 테이블 처럼 continuously 하게 취급한다는 것이다. 이런 새로운 stream processing 모델은 batch processing model에 비해 심플하다. 사용자는 streaming 서비스를 정적 테이블처럼 스트리밍 계산을 표준 배치 처리와 같은 쿼리로 사용한다. 그리고 spark는 한계없는 input table에서 순차적으로 쿼리를 실행한다.
    
   <br/>
### 3-1. Basic Concepts
input data stream은 input table로 간주한다. 모든 데이터 아이템들은 스트림에 도착할때 input table에 새로운 row에 append 된다.   

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark_4.png" width="500px"/>

input 쿼리들은 result table로 변경된다. 모든 트리거 인터벌, 새로운 row들은 input table에 추가되어 결국 result table을 업데이트한다. result table이 업데이트될때마다, 우리는 변경된 결과 행들을 외부싱크에 쓰기를 원할 것이다.    

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark4_2.png" width="500px" />

Output은 어떤게 external storage에 써질지를 정의한다. output은 여러 모드로 정해질 수 있다.    
- complete mode : 전체 업데이트 result table은 외부 스토리지에 모두 작성된다. 이건 스토리지 커넥터에 달려있다.    
- append mode : 마지막 트리거가 외부 스토리지에 써진 이후로부터 새로운 row만 result table에 추가한다. 이건 result table에 있는 rows들이 변경 가능성이 없는 쿼리일 때만 해당한다.    
- update mode : 마지막 트리거가 외부 스토리지에 써진 이후로부터 업데이트된 row만 result table에 추가한다. 이건 complete mode와 다르게 마지막 트리거로 부터 변경된 행만 출력한다는 점이 다르다. 만약 쿼리가 집계 기능을 포함하지 않는다면 이건 append와 동일한 모드이다.    

쿼리 타입에 따라서 모드가 다르게 적용된다.
ine은 dataframe의 input table이고 wordcount는 dataframe의 result table이다. 눈여겨볼점은, streaming lines dataframe 쿼리는 wrodcounts 로 완전히 동일한 정적 dataframe으로 generate 된다. 그러나 이 쿼리가 시작되면, spark 는 소켓 커넥션으로부터 새로운 데이터를 위해 지속적으로 체크한다. 만약 새로운 데이터가 들어온다면 스파크는 증가 쿼리를 시작한다. 이 쿼리는 앞선 러닝 카운트를 새로운 데이터에서 계산된 counts로 조합된다.

<img src="https://github.com/kimhagyeong/Tech_Diary/blob/main/static/spark4_3.png" width="500px" />
structred streaming 은 모든 테이블을 기억하지 않는다는 것을 기억해라. 스트리밍 데이터 소스에서 최근 사용가능한 데이터를 읽고, 결과 업데이트를 위해 증분 처리하고, 소스 데이터를 버린다. 이건 결과를 업데이트하기 위해서 최소한의 중간 상태 데이터만 유지하는 것이다.    
이런 모드는 확연하게 다른 스트리밍 프로세싱 엔진과는 다르다. 많은 스트리밍 시스템은 fault-tolerance와 데이터 일관성을 위해 유저가 집계 러닝을 자체적으로 유지해야한다. 이런 모델에서 스파크는 새로운 데이터가 들어올 때마다 결과 테이블을 업데이트한다는 점에서 의미있고 유저들에게 안심을 준다. 

이외 strcuted streaming 명령어에 대한 블로그 : https://eyeballs.tistory.com/88   
https://moons08.github.io/programming/sparkStreaming/

