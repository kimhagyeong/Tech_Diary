
# Spark SQL, DataFrames and Datasets Guide 
doc : https://spark.apache.org/docs/latest/sql-programming-guide.html
blog :  https://helloino.tistory.com/189?category=770090

<br/><br/>

Spark SQL은 구조화된 데이터 처리를 위한 Spark 모듈이다. spark RDD api 와 다르게 더 많은 information perform, 최적화를 제공한다. 어차피 결과를 계산할 때는 api, sql 상관없이 동일한 실행 엔진이 사용되기에 Spark SQL 과 Dataset API 를 상황에 맞게 선택해 사용하자.
모든 예제는 Spark 배포에 포함된 샘플 데이터를 사용하며 spark-shell, pyspark 쉘 또는 sparkR 쉘에서 실행할 수 있다

<br/><br/>

# 1. SQL

Spark SQL을 사용하여 기존 Hive 설치에서 데이터를 읽을 수 있다. 다른 프로그래밍 언어 내에서 SQL을 실행할 때 결과는 Dataset/DataFrame으로 반환된다. command-line을 사용하거나 JDBC/ODBC를 통해 SQL 인터페이스와 상호 작용할 수도 있다.

<br/><br/>

# 2. Datasets and DataFrames
Dataset은 분산된 데이터 모음이다. Dataset은 spark sql에 최적화된 execution 엔진과 함께 RDD 의 이점(강력한 타이핑, 강력한 람다 함수 사용 기능) 을 제공하는 spark 1.6에 추가된 새로운 인터페이스다.    
Dataset은 JVM 에서 생성되서 map, flatMap, fileter 와 같은 함수들 실행을 제공한다. Dataset API 는 scala 와 java 로 제공. python 서비스는 제공되지 않는데도, python의 동적 제공 이점을 이용해 Dataset API 의 많은 기능을 이미 사용할 수 있다.    

<br/><br/>

DataFrame 은 column으로 구성된 Dataset이다. RDB나 R/python의 dataframe과 컨셉적으로는 동일한 테이블이지만 그 아래 최적화가 더 많이 되어 있다. DataFrame은 구조화된 데이터 파일, Hive의 테이블, 외부 데이터베이스 또는 기존 RDD와 같은 다양한 소스에서 구성할 수 있다.
