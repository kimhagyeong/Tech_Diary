
# Spark SQL, DataFrames and Datasets Guide 
doc : https://spark.apache.org/docs/latest/sql-programming-guide.html
blog :  https://helloino.tistory.com/189?category=770090

<br/><br/>

spark SQL은 구조적 데이터 프로세싱을 위한 spark module 이다. 기본적 spark RDD API 와는 달리, Spark SQL 인터페이스는 더 많은 데이터와 컴퓨팅을 위한  정보를 제공한다. 내부적으로, spark SQL은 추가 정보를 사용해서 더욱 최적화를 수행한다. SQL와 dateset api 을 포함한 Spark SQL에는 몇가지 상호작용이 있다. 결과를 계산하면, 동일한 실행 엔진이 사용되고, API 와 언어에 독립적이게 계산 표현을 할 수 있다. 이러한 통함은 개발자들로 하여금 개발자들이 다루기 편한 transformation 방법의 서로 다른 api 사이에서 더 쉽게 변경할 수 있다는 의미이다.    
이 페이지의 모든 샘플 데이터들은 spark 분산(spark -shell, pyspark shell, sparkR shell 로 동작하는)을 사용하고 있다.


<br/><br/>

# 1. SQL

Spark SQL을 사용하여 기존 Hive 설치에서 데이터를 읽을 수 있다. 다른 프로그래밍 언어 내에서 SQL을 실행할 때 결과는 Dataset/DataFrame으로 반환된다. command-line을 사용하거나 JDBC/ODBC를 통해 SQL 인터페이스와 상호 작용할 수도 있다.

<br/><br/>

# 2. Datasets and DataFrames
Dataset은 분산된 데이터 모음이다. Dataset은 spark sql에 최적화된 execution 엔진과 함께 RDD 의 이점(강력한 타이핑, 강력한 람다 함수 사용 기능) 을 제공하는 spark 1.6에 추가된 새로운 인터페이스다.    
Dataset은 JVM 에서 생성되서 map, flatMap, fileter 와 같은 함수들 실행을 제공한다. Dataset API 는 scala 와 java 로 제공. python 서비스는 제공되지 않는데도, python의 동적 제공 이점을 이용해 Dataset API 의 많은 기능을 이미 사용할 수 있다.    

<br/><br/>

DataFrame 은 column으로 구성된 Dataset이다. RDB나 R/python의 dataframe과 컨셉적으로는 동일한 테이블이지만 그 아래 최적화가 더 많이 되어 있다. DataFrame은 구조화된 데이터 파일, Hive의 테이블, 외부 데이터베이스 또는 기존 RDD와 같은 다양한 소스에서 구성할 수 있다.
