본 내용은 인프런 "스파크 머신러닝 완벽 가이드 - Part 1" 강의를 수강한 내용을 바탕으로 작성하였습니다.

<br/><br/><br/>

#### introduction에서 설명했다시피, Hadoop 에서 Spark 로 넘어가고 있는 상황.    
spark 의 특징을 간략하게
- RDD, DataFrame , SQL 기반의 데이터 처리 추상화
- DAG 기반의 실행 최적화 :     
RDD 특성상 immutable 하고 data transform 작업들이 lineage(혈통) 순서를 가지게 되면서 action 에 최적화된 데이터 변형 그래프를 선택하게 된다.
- In memory 기반 수행으로 Map Reduce 대비 빠른 수행 시간과 자원 최적화


<br/><br/>

#### spark 에서 제공하는 주요 기능
- spark SQL 
- spark streaming 
- MLlib
- GraphX

<br/><br/>


#### 데이터 구조가 RDD, Dataframe, SQL 등으로 정리할 수 있는데, 이 중 DataFrame 에 집중해보도록 할 것
- RDD 개념이 어렵고 spark 의 시작이라서 가장 먼저 배우게 되면서 내용이 많은데,
사실 실무에서는 DataFrame -> SQL 로 작업을 많이 한다.
- pandas 의 numpy 기반 Dataframe과 유사한 형태의 api를 지원하긴하지만(pandas dataframe 사용자를 끌어들이기 위해..),     
최종적으로 SQL을 사용하기 위해 만들어진 것.(따라서 SQL 명령어를 사용해도 dataframe 기반으로 재구성)     
- Dataframe 은 Java Object 로 만들어진 RDD 기반. (왜 java 냐면 scala로 만들어져 있기 때문)

<br/>

- 최종적으로 SQL로 데이터를 처리하고 (SQL 이 더 편리함), DataFrame 자체도 SQL을 염두하고 만들어졌기 때문에 SQL 만 공부하면 되지 않을까 싶지만서도, DataFrame에 대한 학습이 중심이 되어야 한다. 왜냐하면       
    1. 머신러닝 학습 데이터 생성 등 다른 spark component과의 인터페이스를 제공하기 때문이다.
    2. SQL 에서 커버하지 못하는 데이터 처리 기능이 있다.
    3. Loop 기반에서 보다 쉬운 데이터 처리가 가능하다
    4. 사용자 정의 함수를 손쉽게 실행할 수 있기 때문이다.


<br/><br/>

#### Pandas Dataframe 과 spark DataFrame 은 엄연히 다른 API 이다.

- Spark DataFrame 은 SQL 연산과 비슷한 연산자를 제공하고 pandas 는 sum, avg 와 같은 연산에 주로 초점이 맞추어져 있다.
- Spark DataFrame 의 연산은 대부분 새로운 DataFrame 객체를 반환하는 형태로 구성 되어 있다.     
특히 DataFrame 객체에 직접 수정을 허용하지 않고 결과를 다른 객체에 입력해준다. 이건 RDD 의 immutable한 특성 ( scala 의 특성 ) 이 들어나는 항목
- pandas DataFrame 은 [] 를 사용해 특정 컬럼값을 가져올 수 있다. spark dataframe 은 withColumns()

<br/><br/>

#### Spark Dataframe 의 주요 API
- select
- filter
- withColumns
- groupBy


<br/><br/>

#### Spark Dataframe API 더 알아보기
- head, limit
- show, display
- printSchema
- display(df.describe())
- dtypes
- columns
- count
- spark.createDataFrame(pandase_dataframe)
- alias
- filter() => 방식이 여러 개가 있는데, 통합을 위해서 col('column') 을 사용하자
- where
- toPandas
- orderBy, ascending 
- count, max, min, sum
- groupBy, agg
- withColumn
- drop, drop(*drop_columns)* , dropna
- na, isna, isnull, isnan, fillna, replace(float('nan'),None)
- truncate 
- apply, lambda
- when

<br/><br/>

#### Databricks 에 내장된 graph 시각화 기능
spark 의 경우 spark dataframe 을 pandas dataframe으로 변환 후 matplotlib 이나 seaborn 을 적용하여 시각화를 한다.    
그러나 pandas 에서 spark로 바꾼 이유가 메모리에서 데이터를 처리 못할 수도 있기 때문이었는데, spark 가 pandas로 변환할 만큼 데이터 크기가 작아야만 pandas의 시각화 기능을 사용할 수 있다는 이슈가 있다.    
그래도 spark 에서 pandas api 와 유사하게 dataframe을 사용할 수 있도록 해주는 koalas 패키지가 있다. 참고

